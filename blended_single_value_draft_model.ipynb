{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython import display\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn import linear_model, ensemble, tree, feature_selection, model_selection, metrics\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "sns.set_style('white')\n",
    "plt.rcParams['figure.figsize'] = 15,10\n",
    "\n",
    "# Make all columns in dataframe output visible, and display more rows/list items.\n",
    "pd.options.display.max_columns = 999\n",
    "pd.options.display.max_seq_items=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(pd.read_csv('ModelReadyPlayerData.csv'))\n",
    "df = df[pd.notnull(df['ep_PIPM'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.scorer import make_scorer\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def custom_scorer(true_y, pred_y, greater_is_better=True):\n",
    "    arr_true_y = np.array(true_y['ep_PIPM'])\n",
    "    \n",
    "    remove_indices = []\n",
    "    for i in range(len(arr_true_y)):\n",
    "        if arr_true_y[i] == -6:\n",
    "            remove_indices.append(i)\n",
    "\n",
    "    filtered_true_y = np.delete(arr_true_y, remove_indices)\n",
    "    filtered_pred_y = np.delete(pred_y, remove_indices)\n",
    "    \n",
    "    return r2_score(filtered_true_y, filtered_pred_y)\n",
    "\n",
    "filtered_r2 = make_scorer(custom_scorer, greater_is_better=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLA = [\n",
    "\n",
    "    ensemble.AdaBoostRegressor(),\n",
    "    ensemble.BaggingRegressor(),\n",
    "    ensemble.ExtraTreesRegressor(),\n",
    "    ensemble.GradientBoostingRegressor(),\n",
    "    ensemble.RandomForestRegressor(),\n",
    "    \n",
    "    linear_model.RidgeCV(),\n",
    "    \n",
    "    XGBRegressor()\n",
    "]\n",
    "\n",
    "x= ['age_AST%', 'age_BLK%', 'age_DRB%', 'age_DRtg', 'age_ORtg', 'age_PER',\n",
    "        'age_PPR', 'age_PPS', 'age_STL%', 'age_TOV%', 'age_TRB%',\n",
    "       'age_TS%', 'age_Total S %', 'age_USG%', 'age_eFG%', 'age_BPM',\n",
    "       'age_MIN:GP', 'age_USGxTS', 'age_FTRate', 'age_STLK%', 'age_PF:MIN', 'age_PF:STLK', 'age_AST%_ppctl',\n",
    "       'age_BLK%_ppctl', 'age_DRB%_ppctl', 'age_DRtg_ppctl', 'age_ORtg_ppctl',\n",
    "        'age_PPR_ppctl', 'age_PPS_ppctl', 'age_STL%_ppctl',\n",
    "       'age_TOV%_ppctl', 'age_TRB%_ppctl', 'age_TS%_ppctl',\n",
    "       'age_Total S %_ppctl', 'age_USG%_ppctl', 'age_eFG%_ppctl',\n",
    "       'age_BPM_ppctl', 'age_USGxTS_ppctl', 'age_STLK%_ppctl',\n",
    "       'age_PF:MIN_ppctl', 'age_PF:STLK_ppctl', 'age_rFG%', 'age_r3P%',\n",
    "       'age_rFT%', 'Height', 'Weight', 'Height_ppctl', 'Weight_ppctl', 'Age',\n",
    "       'RSCI', 'age_pDBPM_ppctl', 'age_pDBPM', 'age_PER_ppctl', 'age_FTRate_ppctl']\n",
    "\n",
    "y= ['ep_PIPM']\n",
    "\n",
    "# cross validation data split.\n",
    "# run model 10x with 60/30 split intentionally leaving out 10%\n",
    "cv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0 )\n",
    "\n",
    "# Evaluation dataframe for algorithm metrics.\n",
    "MLA_columns = ['MLA Name', 'MLA Train Accuracy Mean', 'MLA Test Accuracy Mean', 'MLA Test Accuracy 3*STD']\n",
    "MLA_compare = pd.DataFrame(columns = MLA_columns)\n",
    "\n",
    "# create table to compare MLA predictions\n",
    "MLA_predict = df[y]\n",
    "print(MLA_predict.shape)\n",
    "\n",
    "# index through MLA and save performance to table\n",
    "row_index = 0\n",
    "\n",
    "for alg in MLA:\n",
    "\n",
    "    print(alg.__class__.__name__)\n",
    "    # set name and parameters\n",
    "    MLA_name = alg.__class__.__name__\n",
    "    MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n",
    "\n",
    "    # score model with cross validation\n",
    "    cv_results = model_selection.cross_validate(alg, df[x], df[y], cv=cv_split) # scoring=filtered_r2\n",
    "\n",
    "    MLA_compare.loc[row_index, 'MLA Train Accuracy Mean'] = cv_results['train_score'].mean()\n",
    "    MLA_compare.loc[row_index, 'MLA Test Accuracy Mean'] = cv_results['test_score'].mean()   \n",
    "    # if this is a non-bias random sample, then +/-3 standard deviations (std) from the mean, should statistically capture 99.7% of the subsets\n",
    "    MLA_compare.loc[row_index, 'MLA Test Accuracy 3*STD'] = cv_results['test_score'].std()*3   # let's know the worst that can happen!\n",
    "\n",
    "    row_index+=1\n",
    "\n",
    "\n",
    "# print and sort table\n",
    "MLA_compare.sort_values(by = ['MLA Test Accuracy Mean'], ascending = False, inplace = True)\n",
    "MLA_compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing Individual Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base model\n",
    "rdg = linear_model.RidgeCV()\n",
    "base_results = model_selection.cross_validate(rdg, df[x], df[y], cv=cv_split, n_jobs=-1)\n",
    "rdg.fit(df[x], df[y])\n",
    "\n",
    "print('BEFORE Parameters: ', rdg.get_params())\n",
    "print(\"BEFORE Training  score mean: {:.2f}\". format(base_results['train_score'].mean()*100)) \n",
    "print(\"BEFORE Test score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\n",
    "print(\"BEFORE Test score 3*std: +/- {:.2f}\". format(base_results['test_score'].std()*100*3))\n",
    "print('-'*10)\n",
    "\n",
    "param_grid = {'alphas': ((165.0,), (170.0,), (175.0,), (180.0,), (190.0,), (200.0,))\n",
    "             }\n",
    "tune_model = model_selection.GridSearchCV(linear_model.RidgeCV(), param_grid=param_grid, cv=cv_split, n_jobs=-1)\n",
    "tune_model.fit(df[x], df[y])\n",
    "\n",
    "\n",
    "print('AFTER Parameters: ', tune_model.best_params_)\n",
    "print(\"AFTER Training score mean: {:.2f}\". format(tune_model.cv_results_['mean_train_score'][tune_model.best_index_]*100)) \n",
    "print(\"AFTER Test score mean: {:.2f}\". format(tune_model.cv_results_['mean_test_score'][tune_model.best_index_]*100))\n",
    "print(\"AFTER Test score 3*std: +/- {:.2f}\". format(tune_model.cv_results_['std_test_score'][tune_model.best_index_]*100*3))\n",
    "print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Feature Tuning\n",
    "rdg = linear_model.RidgeCV(alphas=(175.0,))\n",
    "base_results = model_selection.cross_validate(rdg, df[x], df[y], cv=cv_split, n_jobs=-1)\n",
    "rdg.fit(df[x], df[y])\n",
    "\n",
    "\n",
    "print('BEFORE RFE Training Shape Old: ', df[x].shape) \n",
    "print('BEFORE RFE Training Columns Old: ', df[x].columns.values)\n",
    "\n",
    "print(\"BEFORE RFE Training score mean: {:.2f}\". format(base_results['train_score'].mean()*100)) \n",
    "print(\"BEFORE RFE Test score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\n",
    "print(\"BEFORE RFE Test score 3*std: +/- {:.2f}\". format(base_results['test_score'].std()*100*3))\n",
    "print('-'*10)\n",
    "\n",
    "\n",
    "\n",
    "#feature selection\n",
    "rdg_rfe = feature_selection.RFECV(rdg, step = 1, cv = cv_split, n_jobs=-1)\n",
    "rdg_rfe.fit(df[x], df[y])\n",
    "\n",
    "#transform x&y to reduced features and fit new model\n",
    "X_rfe = df[x].columns.values[rdg_rfe.get_support()]\n",
    "rfe_results = model_selection.cross_validate(rdg, df[X_rfe], df[y], cv  = cv_split, n_jobs=-1)\n",
    "\n",
    "print('AFTER RFE Training Shape New: ', df[X_rfe].shape) \n",
    "print('AFTER RFE Training Columns New: ', X_rfe)\n",
    "\n",
    "print(\"AFTER RFE Training score mean: {:.2f}\". format(rfe_results['train_score'].mean()*100)) \n",
    "print(\"AFTER RFE Test score mean: {:.2f}\". format(rfe_results['test_score'].mean()*100))\n",
    "print(\"AFTER RFE Test score 3*std: +/- {:.2f}\". format(rfe_results['test_score'].std()*100*3))\n",
    "print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdg_x = ['age_BLK%', 'age_DRB%', 'age_DRtg', 'age_PER', 'age_PPR', 'age_STL%', 'age_TRB%',\n",
    " 'age_BPM', 'age_MIN:GP', 'age_USGxTS', 'age_FTRate', 'age_STLK%',\n",
    " 'age_DRB%_ppctl', 'age_TRB%_ppctl', 'age_rFG%', 'Height', 'Weight_ppctl', 'Age',\n",
    " 'age_pDBPM_ppctl', 'age_pDBPM']\n",
    "\n",
    "#base model\n",
    "rdg = linear_model.RidgeCV(alphas=(175.0,))\n",
    "base_results = model_selection.cross_validate(rdg, df[rdg_x], df[y], cv=cv_split, n_jobs=-1)\n",
    "rdg.fit(df[rdg_x], df[y])\n",
    "\n",
    "print('BEFORE Parameters: ', rdg.get_params())\n",
    "print(\"BEFORE Training  score mean: {:.2f}\". format(base_results['train_score'].mean()*100)) \n",
    "print(\"BEFORE Test score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\n",
    "print(\"BEFORE Test score 3*std: +/- {:.2f}\". format(base_results['test_score'].std()*100*3))\n",
    "print('-'*10)\n",
    "\n",
    "param_grid = {'alphas': ((110.0,), (115.0,), (90.0,), (95.0,), (100.0,), (105.0,))\n",
    "             }\n",
    "tune_model = model_selection.GridSearchCV(linear_model.RidgeCV(), param_grid=param_grid, cv=cv_split, n_jobs=-1)\n",
    "tune_model.fit(df[rdg_x], df[y])\n",
    "\n",
    "\n",
    "print('AFTER Parameters: ', tune_model.best_params_)\n",
    "print(\"AFTER Training score mean: {:.2f}\". format(tune_model.cv_results_['mean_train_score'][tune_model.best_index_]*100)) \n",
    "print(\"AFTER Test score mean: {:.2f}\". format(tune_model.cv_results_['mean_test_score'][tune_model.best_index_]*100))\n",
    "print(\"AFTER Test score 3*std: +/- {:.2f}\". format(tune_model.cv_results_['std_test_score'][tune_model.best_index_]*100*3))\n",
    "print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final ridge\n",
    "rdg_x = ['age_BLK%', 'age_DRB%', 'age_DRtg', 'age_PER', 'age_PPR', 'age_STL%', 'age_TRB%',\n",
    " 'age_BPM', 'age_MIN:GP', 'age_USGxTS', 'age_FTRate', 'age_STLK%',\n",
    " 'age_DRB%_ppctl', 'age_TRB%_ppctl', 'age_rFG%', 'Height', 'Weight_ppctl', 'Age',\n",
    " 'age_pDBPM_ppctl', 'age_pDBPM']\n",
    "\n",
    "#base model\n",
    "rdg = linear_model.RidgeCV(alphas=(110.0,))\n",
    "base_results = model_selection.cross_validate(rdg, df[rdg_x], df[y], cv=cv_split, n_jobs=-1)\n",
    "rdg.fit(df[rdg_x], df[y])\n",
    "\n",
    "print('Parameters: ', tune_model.best_params_)\n",
    "print(\"Training score mean: {:.2f}\". format(tune_model.cv_results_['mean_train_score'][tune_model.best_index_]*100)) \n",
    "print(\"Test score mean: {:.2f}\". format(tune_model.cv_results_['mean_test_score'][tune_model.best_index_]*100))\n",
    "print(\"Test score 3*std: +/- {:.2f}\". format(tune_model.cv_results_['std_test_score'][tune_model.best_index_]*100*3))\n",
    "print('-'*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model selection process with cross validation.\n",
    "cv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0 ) \n",
    "# run model 10x with 60/30 split intentionally leaving out 10%\n",
    "\n",
    "#base model\n",
    "grb = ensemble.GradientBoostingRegressor(random_state=2)\n",
    "base_results = model_selection.cross_validate(grb, df[x], df[y], cv=cv_split, n_jobs=-1)\n",
    "grb.fit(df[x], df[y])\n",
    "\n",
    "\n",
    "print('BEFORE Parameters: ', grb.get_params())\n",
    "print(\"BEFORE Training score mean: {:.2f}\". format(base_results['train_score'].mean()*100)) \n",
    "print(\"BEFORE Test score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\n",
    "print(\"BEFORE Test score 3*std: +/- {:.2f}\". format(base_results['test_score'].std()*100*3))\n",
    "print(\"BEFORE Test set score min: {:.2f}\". format(base_results['test_score'].min()*100))\n",
    "print('-'*10)\n",
    "\n",
    "\n",
    "#tune hyper-parameters: \n",
    "param_grid = {\n",
    "                'max_depth': [3, 4, 5],\n",
    "                'n_estimators': [35, 40, 45],\n",
    "                'min_samples_split': [2],\n",
    "                'min_samples_leaf': [9, 10, 11],\n",
    "                'subsample': [.7, .75, .8],\n",
    "                'max_features': [None, .1, .2, .3]\n",
    "             }\n",
    "\n",
    "\n",
    "# choose best model with grid_search: \n",
    "tune_model = model_selection.GridSearchCV(grb, param_grid=param_grid, cv=cv_split, n_jobs=-1)\n",
    "tune_model.fit(df[x], df[y])\n",
    "\n",
    "print('AFTER Parameters: ', tune_model.best_params_)\n",
    "print(\"AFTER Training score mean: {:.2f}\". format(tune_model.cv_results_['mean_train_score'][tune_model.best_index_]*100)) \n",
    "print(\"AFTER Test score mean: {:.2f}\". format(tune_model.cv_results_['mean_test_score'][tune_model.best_index_]*100))\n",
    "print(\"AFTER Test score 3*std: +/- {:.2f}\". format(tune_model.cv_results_['std_test_score'][tune_model.best_index_]*100*3))\n",
    "print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Feature Tuning\n",
    "\n",
    "#base model\n",
    "grb = ensemble.GradientBoostingRegressor(max_depth=3, min_samples_leaf=10, n_estimators=40, max_features=.1, subsample=.75)\n",
    "base_results = model_selection.cross_validate(grb, df[x], df[y], cv=cv_split, n_jobs=-1)\n",
    "grb.fit(df[x], df[y])\n",
    "\n",
    "print('BEFORE RFE Training Shape Old: ', df[x].shape) \n",
    "print('BEFORE RFE Training Columns Old: ', df[x].columns.values)\n",
    "\n",
    "print(\"BEFORE RFE Training score mean: {:.2f}\". format(base_results['train_score'].mean()*100)) \n",
    "print(\"BEFORE RFE Test score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\n",
    "print(\"BEFORE RFE Test score 3*std: +/- {:.2f}\". format(base_results['test_score'].std()*100*3))\n",
    "print('-'*10)\n",
    "\n",
    "\n",
    "\n",
    "#feature selection\n",
    "grb_rfe = feature_selection.RFECV(grb, step = 1, cv = cv_split, n_jobs=-1)\n",
    "grb_rfe.fit(df[x], df[y])\n",
    "\n",
    "#transform x&y to reduced features and fit new model\n",
    "X_rfe = df[x].columns.values[grb_rfe.get_support()]\n",
    "rfe_results = model_selection.cross_validate(grb, df[X_rfe], df[y], cv  = cv_split, n_jobs=-1)\n",
    "\n",
    "print('AFTER RFE Training Shape New: ', df[X_rfe].shape) \n",
    "print('AFTER RFE Training Columns New: ', X_rfe)\n",
    "\n",
    "print(\"AFTER RFE Training score mean: {:.2f}\". format(rfe_results['train_score'].mean()*100)) \n",
    "print(\"AFTER RFE Test score mean: {:.2f}\". format(rfe_results['test_score'].mean()*100))\n",
    "print(\"AFTER RFE Test score 3*std: +/- {:.2f}\". format(rfe_results['test_score'].std()*100*3))\n",
    "print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grb_x = ['age_AST%', 'age_DRB%', 'age_DRtg', 'age_ORtg', 'age_PER', 'age_PPR', 'age_PPS',\n",
    " 'age_STL%', 'age_TOV%', 'age_TRB%', 'age_TS%', 'age_Total S %', 'age_eFG%',\n",
    " 'age_BPM', 'age_MIN:GP', 'age_USGxTS', 'age_FTRate', 'age_PF:STLK',\n",
    " 'age_AST%_ppctl', 'age_BLK%_ppctl', 'age_DRB%_ppctl', 'age_DRtg_ppctl',\n",
    " 'age_ORtg_ppctl', 'age_PPR_ppctl', 'age_PPS_ppctl', 'age_STL%_ppctl',\n",
    " 'age_TS%_ppctl', 'age_Total S %_ppctl', 'age_USG%_ppctl', 'age_eFG%_ppctl',\n",
    " 'age_BPM_ppctl', 'age_USGxTS_ppctl', 'age_STLK%_ppctl', 'age_PF:STLK_ppctl',\n",
    " 'age_rFG%', 'age_rFT%', 'Height_ppctl', 'Weight_ppctl', 'Age',\n",
    " 'age_pDBPM_ppctl', 'age_PER_ppctl']\n",
    "\n",
    "# final grb\n",
    "grb = ensemble.GradientBoostingRegressor(max_depth=3, min_samples_leaf=10, n_estimators=40, max_features=.1, subsample=.75)\n",
    "base_results = model_selection.cross_validate(grb, df[grb_x], df[y], cv=cv_split, n_jobs=-1)\n",
    "grb.fit(df[grb_x], df[y])\n",
    "\n",
    "\n",
    "print('Parameters: ', grb.get_params())\n",
    "print(\"Training score mean: {:.2f}\". format(base_results['train_score'].mean()*100)) \n",
    "print(\"Test score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\n",
    "print(\"Test score 3*std: +/- {:.2f}\". format(base_results['test_score'].std()*100*3))\n",
    "print(\"Test set score min: {:.2f}\". format(base_results['test_score'].min()*100))\n",
    "print('-'*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XG Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model selection process with cross validation.\n",
    "cv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0 ) \n",
    "# run model 10x with 60/30 split intentionally leaving out 10%\n",
    "\n",
    "#base model\n",
    "xgb = XGBRegressor()\n",
    "base_results = model_selection.cross_validate(xgb, df[x], df[y], cv=cv_split, n_jobs=-1)\n",
    "xgb.fit(df[x], df[y])\n",
    "\n",
    "\n",
    "print('BEFORE Parameters: ', xgb.get_params())\n",
    "print(\"BEFORE Training score mean: {:.2f}\". format(base_results['train_score'].mean()*100)) \n",
    "print(\"BEFORE Test score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\n",
    "print(\"BEFORE Test score 3*std: +/- {:.2f}\". format(base_results['test_score'].std()*100*3))\n",
    "print(\"BEFORE Test set score min: {:.2f}\". format(base_results['test_score'].min()*100))\n",
    "print('-'*10)\n",
    "\n",
    "\n",
    "#tune hyper-parameters: \n",
    "param_grid = {\n",
    "                'max_depth': [3, 4, 5],\n",
    "                'n_estimators': [25, 30, 35],\n",
    "                'subsample': [.8, .85, .9, .95, 1.0],\n",
    "                'colsample_bylevel': [.2, .25, .3],\n",
    "                'colsample_bytree': [.2, .25, .3]\n",
    "             }\n",
    "\n",
    "# choose best model with grid_search: \n",
    "tune_model = model_selection.GridSearchCV(XGBRegressor(), param_grid=param_grid, cv=cv_split, n_jobs=-1)\n",
    "tune_model.fit(df[x], df[y])\n",
    "\n",
    "\n",
    "print('AFTER Parameters: ', tune_model.best_params_)\n",
    "print(\"AFTER Training score mean: {:.2f}\". format(tune_model.cv_results_['mean_train_score'][tune_model.best_index_]*100)) \n",
    "print(\"AFTER Test score mean: {:.2f}\". format(tune_model.cv_results_['mean_test_score'][tune_model.best_index_]*100))\n",
    "print(\"AFTER Test score 3*std: +/- {:.2f}\". format(tune_model.cv_results_['std_test_score'][tune_model.best_index_]*100*3))\n",
    "print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#base model\n",
    "xgb = XGBRegressor(colsample_bylevel=0.25, colsample_bytree=0.25, max_depth=4, n_estimators=30)\n",
    "base_results = model_selection.cross_validate(xgb, df[x], df[y], cv=cv_split, n_jobs=-1)\n",
    "xgb.fit(df[x], df[y])\n",
    "\n",
    "\n",
    "print('Parameters: ', xgb.get_params())\n",
    "print(\"Training score mean: {:.2f}\". format(base_results['train_score'].mean()*100)) \n",
    "print(\"Test score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\n",
    "print(\"Test score 3*std: +/- {:.2f}\". format(base_results['test_score'].std()*100*3))\n",
    "print(\"Test set score min: {:.2f}\". format(base_results['test_score'].min()*100))\n",
    "print('-'*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base model\n",
    "bag = ensemble.BaggingRegressor()\n",
    "base_results = model_selection.cross_validate(bag, df[x], df[y], cv=cv_split, n_jobs=-1)\n",
    "bag.fit(df[x], df[y])\n",
    "\n",
    "\n",
    "print('BEFORE Parameters: ', bag.get_params())\n",
    "print(\"BEFORE Training  score mean: {:.2f}\". format(base_results['train_score'].mean()*100)) \n",
    "print(\"BEFORE Test score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\n",
    "print(\"BEFORE Test score 3*std: +/- {:.2f}\". format(base_results['test_score'].std()*100*3))\n",
    "print(\"BEFORE Test set score min: {:.2f}\". format(base_results['test_score'].min()*100))\n",
    "print('-'*10)\n",
    "\n",
    "\n",
    "#tune hyper-parameters: \n",
    "param_grid = {\n",
    "                'base_estimator': [ensemble.GradientBoostingRegressor(n_estimators=125, max_depth=3, min_samples_split=7, min_samples_leaf=1)],\n",
    "                'n_estimators': [35],\n",
    "                'max_samples': [.4],\n",
    "                'bootstrap_features': [False],\n",
    "                'random_state': [2]\n",
    "             }\n",
    "\n",
    "\n",
    "# choose best model with grid_search: \n",
    "tune_model = model_selection.GridSearchCV(ensemble.BaggingRegressor(), param_grid=param_grid, cv=cv_split, n_jobs=-1)\n",
    "tune_model.fit(df[x], df[y])\n",
    "\n",
    "print('AFTER DT Parameters: ', tune_model.best_params_)\n",
    "print(\"AFTER DT Training score mean: {:.2f}\". format(tune_model.cv_results_['mean_train_score'][tune_model.best_index_]*100)) \n",
    "print(\"AFTER DT Test score mean: {:.2f}\". format(tune_model.cv_results_['mean_test_score'][tune_model.best_index_]*100))\n",
    "print(\"AFTER DT Test score 3*std: +/- {:.2f}\". format(tune_model.cv_results_['std_test_score'][tune_model.best_index_]*100*3))\n",
    "print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final model\n",
    "bag = ensemble.BaggingRegressor(ensemble.GradientBoostingRegressor(n_estimators=125, max_depth=3, min_samples_split=7),\n",
    "                                n_estimators=35,\n",
    "                                max_samples=.4,\n",
    "                                bootstrap_features=False)\n",
    "base_results = model_selection.cross_validate(bag, df[x], df[y], cv=cv_split, n_jobs=-1)\n",
    "bag.fit(df[x], df[y])\n",
    "\n",
    "print(\"Training score mean: {:.2f}\". format(base_results['train_score'].mean()*100)) \n",
    "print(\"Test score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\n",
    "print(\"Test score 3*std: +/- {:.2f}\". format(base_results['test_score'].std()*100*3))\n",
    "print('-'*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaboost (base: decision tree regressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base model\n",
    "ada = ensemble.AdaBoostRegressor()\n",
    "base_results = model_selection.cross_validate(ada, df[x], df[y], cv=cv_split, n_jobs=-1)\n",
    "ada.fit(df[x], df[y])\n",
    "\n",
    "\n",
    "print('BEFORE Parameters: ', ada.get_params())\n",
    "print(\"BEFORE Training  score mean: {:.2f}\". format(base_results['train_score'].mean()*100)) \n",
    "print(\"BEFORE Test score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\n",
    "print(\"BEFORE Test score 3*std: +/- {:.2f}\". format(base_results['test_score'].std()*100*3))\n",
    "print(\"BEFORE Test set score min: {:.2f}\". format(base_results['test_score'].min()*100))\n",
    "print('-'*10)\n",
    "\n",
    "\n",
    "#tune hyper-parameters: \n",
    "param_grid = {\n",
    "                'base_estimator': [tree.DecisionTreeRegressor(max_depth=5)],\n",
    "                'n_estimators': [60],\n",
    "                'random_state': [2]\n",
    "             }\n",
    "\n",
    "\n",
    "# choose best model with grid_search: \n",
    "tune_model = model_selection.GridSearchCV(ensemble.AdaBoostRegressor(), param_grid=param_grid, cv=cv_split, n_jobs=-1)\n",
    "tune_model.fit(df[x], df[y])\n",
    "\n",
    "print('AFTER Parameters: ', tune_model.best_params_)\n",
    "print(\"AFTER Training score mean: {:.2f}\". format(tune_model.cv_results_['mean_train_score'][tune_model.best_index_]*100)) \n",
    "print(\"AFTER Test score mean: {:.2f}\". format(tune_model.cv_results_['mean_test_score'][tune_model.best_index_]*100))\n",
    "print(\"AFTER Test score 3*std: +/- {:.2f}\". format(tune_model.cv_results_['std_test_score'][tune_model.best_index_]*100*3))\n",
    "print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Feature Tuning\n",
    "\n",
    "#base model\n",
    "ada = ensemble.AdaBoostRegressor(tree.DecisionTreeRegressor(max_depth=5), n_estimators=60)\n",
    "base_results = model_selection.cross_validate(ada, df[x], df[y], cv=cv_split, n_jobs=-1)\n",
    "ada.fit(df[x], df[y])\n",
    "\n",
    "print('BEFORE RFE Training Shape Old: ', df[x].shape) \n",
    "print('BEFORE RFE Training Columns Old: ', df[x].columns.values)\n",
    "\n",
    "print(\"BEFORE RFE Training score mean: {:.2f}\". format(base_results['train_score'].mean()*100)) \n",
    "print(\"BEFORE RFE Test score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\n",
    "print(\"BEFORE RFE Test score 3*std: +/- {:.2f}\". format(base_results['test_score'].std()*100*3))\n",
    "print('-'*10)\n",
    "\n",
    "\n",
    "\n",
    "#feature selection\n",
    "ada_rfe = feature_selection.RFECV(ada, step = 1, cv = cv_split, n_jobs=-1)\n",
    "ada_rfe.fit(df[x], df[y])\n",
    "\n",
    "#transform x&y to reduced features and fit new model\n",
    "X_rfe = df[x].columns.values[ada_rfe.get_support()]\n",
    "rfe_results = model_selection.cross_validate(ada, df[X_rfe], df[y], cv  = cv_split, n_jobs=-1)\n",
    "\n",
    "print('AFTER RFE Training Shape New: ', df[X_rfe].shape) \n",
    "print('AFTER RFE Training Columns New: ', X_rfe)\n",
    "\n",
    "print(\"AFTER RFE Training score mean: {:.2f}\". format(rfe_results['train_score'].mean()*100)) \n",
    "print(\"AFTER RFE Test score mean: {:.2f}\". format(rfe_results['test_score'].mean()*100))\n",
    "print(\"AFTER RFE Test score 3*std: +/- {:.2f}\". format(rfe_results['test_score'].std()*100*3))\n",
    "print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ada_x = ['age_AST%', 'age_DRB%', 'age_DRtg', 'age_ORtg', 'age_PER', 'age_PPR', 'age_STL%',\n",
    " 'age_BPM', 'age_MIN:GP', 'age_USGxTS', 'age_STLK%', 'age_BLK%_ppctl',\n",
    " 'age_DRtg_ppctl', 'age_PPR_ppctl', 'age_PPS_ppctl', 'age_STL%_ppctl',\n",
    " 'age_TOV%_ppctl', 'age_TRB%_ppctl', 'age_USG%_ppctl', 'age_eFG%_ppctl',\n",
    " 'age_BPM_ppctl', 'age_PF:STLK_ppctl', 'age_rFG%', 'age_r3P%', 'age_rFT%',\n",
    " 'Height_ppctl', 'Weight_ppctl', 'Age', 'RSCI', 'age_pDBPM_ppctl', 'age_pDBPM',\n",
    " 'age_PER_ppctl', 'age_FTRate_ppctl']\n",
    "\n",
    "#base model\n",
    "ada = ensemble.AdaBoostRegressor(tree.DecisionTreeRegressor(max_depth=5), n_estimators=60)\n",
    "base_results = model_selection.cross_validate(ada, df[ada_x], df[y], cv=cv_split, n_jobs=-1)\n",
    "ada.fit(df[ada_x], df[y])\n",
    "\n",
    "\n",
    "print('BEFORE Parameters: ', ada.get_params())\n",
    "print(\"BEFORE Training score mean: {:.2f}\". format(base_results['train_score'].mean()*100)) \n",
    "print(\"BEFORE Test score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\n",
    "print(\"BEFORE Test score 3*std: +/- {:.2f}\". format(base_results['test_score'].std()*100*3))\n",
    "print(\"BEFORE Test set score min: {:.2f}\". format(base_results['test_score'].min()*100))\n",
    "print('-'*10)\n",
    "\n",
    "\n",
    "#tune hyper-parameters: \n",
    "param_grid = {\n",
    "                'base_estimator': [tree.DecisionTreeRegressor(max_depth=5, max_features=None),\n",
    "                                   tree.DecisionTreeRegressor(max_depth=5, max_features=.1),\n",
    "                                   tree.DecisionTreeRegressor(max_depth=5, max_features=.2),\n",
    "                                   tree.DecisionTreeRegressor(max_depth=5, max_features=.3),\n",
    "                                   tree.DecisionTreeRegressor(max_depth=5, max_features=.4),\n",
    "                                   tree.DecisionTreeRegressor(max_depth=5, max_features=.5),\n",
    "                                   tree.DecisionTreeRegressor(max_depth=5, max_features=.6),\n",
    "                                   tree.DecisionTreeRegressor(max_depth=5, max_features=.7),\n",
    "                                   tree.DecisionTreeRegressor(max_depth=5, max_features=.8),\n",
    "                                   tree.DecisionTreeRegressor(max_depth=5, max_features=.9)],\n",
    "                'n_estimators': [60],\n",
    "                'random_state': [2]\n",
    "             }\n",
    "\n",
    "# choose best model with grid_search: \n",
    "tune_model = model_selection.GridSearchCV(ensemble.AdaBoostRegressor(), param_grid=param_grid, cv=cv_split, n_jobs=-1)\n",
    "tune_model.fit(df[ada_x], df[y])\n",
    "\n",
    "print('AFTER DT Parameters: ', tune_model.best_params_)\n",
    "print(\"AFTER DT Training score mean: {:.2f}\". format(tune_model.cv_results_['mean_train_score'][tune_model.best_index_]*100)) \n",
    "print(\"AFTER DT Test score mean: {:.2f}\". format(tune_model.cv_results_['mean_test_score'][tune_model.best_index_]*100))\n",
    "print(\"AFTER DT Test score 3*std: +/- {:.2f}\". format(tune_model.cv_results_['std_test_score'][tune_model.best_index_]*100*3))\n",
    "print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_x = ['age_AST%', 'age_DRB%', 'age_DRtg', 'age_ORtg', 'age_PER', 'age_PPR', 'age_STL%',\n",
    " 'age_BPM', 'age_MIN:GP', 'age_USGxTS', 'age_STLK%', 'age_BLK%_ppctl',\n",
    " 'age_DRtg_ppctl', 'age_PPR_ppctl', 'age_PPS_ppctl', 'age_STL%_ppctl',\n",
    " 'age_TOV%_ppctl', 'age_TRB%_ppctl', 'age_USG%_ppctl', 'age_eFG%_ppctl',\n",
    " 'age_BPM_ppctl', 'age_PF:STLK_ppctl', 'age_rFG%', 'age_r3P%', 'age_rFT%',\n",
    " 'Height_ppctl', 'Weight_ppctl', 'Age', 'RSCI', 'age_pDBPM_ppctl', 'age_pDBPM',\n",
    " 'age_PER_ppctl', 'age_FTRate_ppctl']\n",
    "\n",
    "#final model\n",
    "ada = ensemble.AdaBoostRegressor(tree.DecisionTreeRegressor(max_depth=5, max_features=0.2), n_estimators=60)\n",
    "base_results = model_selection.cross_validate(ada, df[ada_x], df[y], cv=cv_split, n_jobs=-1)\n",
    "ada.fit(df[ada_x], df[y])\n",
    "\n",
    "\n",
    "print('Parameters: ', ada.get_params())\n",
    "print(\"Training score mean: {:.2f}\". format(base_results['train_score'].mean()*100)) \n",
    "print(\"Test score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\n",
    "print(\"Test score 3*std: +/- {:.2f}\". format(base_results['test_score'].std()*100*3))\n",
    "print(\"Test set score min: {:.2f}\". format(base_results['test_score'].min()*100))\n",
    "print('-'*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base model\n",
    "rfr = ensemble.RandomForestRegressor(random_state=2, n_jobs=-1)\n",
    "base_results = model_selection.cross_validate(rfr, df[x], df[y], cv=cv_split, n_jobs=-1)\n",
    "rfr.fit(df[x], df[y])\n",
    "\n",
    "\n",
    "print('BEFORE Parameters: ', rfr.get_params())\n",
    "print(\"BEFORE Training score mean: {:.2f}\". format(base_results['train_score'].mean()*100)) \n",
    "print(\"BEFORE DT Test score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\n",
    "print(\"BEFORE DT Test score 3*std: +/- {:.2f}\". format(base_results['test_score'].std()*100*3))\n",
    "print(\"BEFORE DT Test w/bin set score min: {:.2f}\". format(base_results['test_score'].min()*100))\n",
    "print('-'*10)\n",
    "\n",
    "\n",
    "#tune hyper-parameters: \n",
    "param_grid = {\n",
    "                'n_estimators': [750, 800, 850],\n",
    "                'max_features': [.25, .3, .35],\n",
    "                'min_samples_split': [10, 12, 14],\n",
    "                'min_samples_leaf': [1],\n",
    "                'max_leaf_nodes': [17, 18, 19],\n",
    "                'bootstrap': [True],\n",
    "                'random_state': [2]\n",
    "             }\n",
    "\n",
    "\n",
    "# choose best model with grid_search: \n",
    "tune_model = model_selection.GridSearchCV(rfr, param_grid=param_grid, cv=cv_split, n_jobs=-1)\n",
    "tune_model.fit(df[x], df[y])\n",
    "\n",
    "print('AFTER Parameters: ', tune_model.best_params_)\n",
    "print(\"AFTER Training score mean: {:.2f}\". format(tune_model.cv_results_['mean_train_score'][tune_model.best_index_]*100)) \n",
    "print(\"AFTER Test score mean: {:.2f}\". format(tune_model.cv_results_['mean_test_score'][tune_model.best_index_]*100))\n",
    "print(\"AFTER Test score 3*std: +/- {:.2f}\". format(tune_model.cv_results_['std_test_score'][tune_model.best_index_]*100*3))\n",
    "print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature tuning\n",
    "\n",
    "#base model\n",
    "rfr = ensemble.RandomForestRegressor(bootstrap=True, \n",
    "                                     max_features=0.3, \n",
    "                                     max_leaf_nodes=18,\n",
    "                                     min_samples_split=12, \n",
    "                                     n_estimators=800,\n",
    "                                     random_state=2, n_jobs=-1)\n",
    "base_results = model_selection.cross_validate(rfr, df[x], df[y], cv=cv_split, n_jobs=-1)\n",
    "rfr.fit(df[x], df[y])\n",
    "\n",
    "print('BEFORE RFE Training Shape Old: ', df[x].shape) \n",
    "print('BEFORE RFE Training Columns Old: ', df[x].columns.values)\n",
    "\n",
    "print(\"BEFORE RFE Training score mean: {:.2f}\". format(base_results['train_score'].mean()*100)) \n",
    "print(\"BEFORE RFE Test score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\n",
    "print(\"BEFORE RFE Test score 3*std: +/- {:.2f}\". format(base_results['test_score'].std()*100*3))\n",
    "print('-'*10)\n",
    "\n",
    "\n",
    "\n",
    "#feature selection\n",
    "rfr_rfe = feature_selection.RFECV(rfr, step = 1, cv = cv_split, n_jobs=-1)\n",
    "rfr_rfe.fit(df[x], df[y])\n",
    "\n",
    "#transform x&y to reduced features and fit new model\n",
    "X_rfe = df[x].columns.values[rfr_rfe.get_support()]\n",
    "rfe_results = model_selection.cross_validate(rfr, df[X_rfe], df[y], cv  = cv_split, n_jobs=-1)\n",
    "\n",
    "print('AFTER RFE Training Shape New: ', df[X_rfe].shape) \n",
    "print('AFTER RFE Training Columns New: ', X_rfe)\n",
    "\n",
    "print(\"AFTER RFE Training score mean: {:.2f}\". format(rfe_results['train_score'].mean()*100)) \n",
    "print(\"AFTER RFE Test score mean: {:.2f}\". format(rfe_results['test_score'].mean()*100))\n",
    "print(\"AFTER RFE Test score 3*std: +/- {:.2f}\". format(rfe_results['test_score'].std()*100*3))\n",
    "print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr_x = ['age_DRtg', 'age_ORtg', 'age_PER', 'age_PPR', 'age_STL%', 'age_BPM',\n",
    " 'age_DRB%_ppctl', 'age_DRtg_ppctl', 'age_ORtg_ppctl', 'age_PPR_ppctl',\n",
    " 'age_STL%_ppctl', 'age_TS%_ppctl', 'age_eFG%_ppctl', 'age_BPM_ppctl',\n",
    " 'age_rFT%', 'Height_ppctl', 'Age', 'age_pDBPM_ppctl', 'age_pDBPM',\n",
    " 'age_PER_ppctl']\n",
    "\n",
    "#base model\n",
    "rfr = ensemble.RandomForestRegressor(random_state=2, n_jobs=-1)\n",
    "base_results = model_selection.cross_validate(rfr, df[rfr_x], df[y], cv=cv_split, n_jobs=-1)\n",
    "rfr.fit(df[rfr_x], df[y])\n",
    "\n",
    "\n",
    "print('BEFORE Parameters: ', rfr.get_params())\n",
    "print(\"BEFORE Training score mean: {:.2f}\". format(base_results['train_score'].mean()*100)) \n",
    "print(\"BEFORE Test score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\n",
    "print(\"BEFORE Test score 3*std: +/- {:.2f}\". format(base_results['test_score'].std()*100*3))\n",
    "print(\"BEFORE Test set score min: {:.2f}\". format(base_results['test_score'].min()*100))\n",
    "print('-'*10)\n",
    "\n",
    "\n",
    "#tune hyper-parameters: \n",
    "param_grid = {\n",
    "                'n_estimators': [850, 900, 1000],\n",
    "                'max_features': [.35, .4, .45],\n",
    "                'min_samples_split': [6, 8, 10],\n",
    "                'min_samples_leaf': [1],\n",
    "                'max_leaf_nodes': [19, 20, 21],\n",
    "                'bootstrap': [True],\n",
    "                'random_state': [2]\n",
    "             }\n",
    "\n",
    "\n",
    "# choose best model with grid_search: \n",
    "tune_model = model_selection.GridSearchCV(rfr, param_grid=param_grid, cv=cv_split, n_jobs=-1)\n",
    "tune_model.fit(df[rfr_x], df[y])\n",
    "\n",
    "print('AFTER Parameters: ', tune_model.best_params_)\n",
    "print(\"AFTER Training score mean: {:.2f}\". format(tune_model.cv_results_['mean_train_score'][tune_model.best_index_]*100)) \n",
    "print(\"AFTER Test score mean: {:.2f}\". format(tune_model.cv_results_['mean_test_score'][tune_model.best_index_]*100))\n",
    "print(\"AFTER Test score 3*std: +/- {:.2f}\". format(tune_model.cv_results_['std_test_score'][tune_model.best_index_]*100*3))\n",
    "print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr_x = ['age_DRtg', 'age_ORtg', 'age_PER', 'age_PPR', 'age_STL%', 'age_BPM',\n",
    " 'age_DRB%_ppctl', 'age_DRtg_ppctl', 'age_ORtg_ppctl', 'age_PPR_ppctl',\n",
    " 'age_STL%_ppctl', 'age_TS%_ppctl', 'age_eFG%_ppctl', 'age_BPM_ppctl',\n",
    " 'age_rFT%', 'Height_ppctl', 'Age', 'age_pDBPM_ppctl', 'age_pDBPM',\n",
    " 'age_PER_ppctl']\n",
    "\n",
    "# final model\n",
    "rfr = ensemble.RandomForestRegressor(bootstrap=True, \n",
    "                                     max_features=0.35, \n",
    "                                     max_leaf_nodes=19,\n",
    "                                     min_samples_split=10, \n",
    "                                     n_estimators=850,\n",
    "                                     random_state=2, n_jobs=-1)\n",
    "base_results = model_selection.cross_validate(rfr, df[rfr_x], df[y], cv=cv_split, n_jobs=-1)\n",
    "rfr.fit(df[rfr_x], df[y])\n",
    "\n",
    "print('Parameters: ', rfr.get_params())\n",
    "print(\"Training score mean: {:.2f}\". format(base_results['train_score'].mean()*100)) \n",
    "print(\"Test score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\n",
    "print(\"Test score 3*std: +/- {:.2f}\". format(base_results['test_score'].std()*100*3))\n",
    "print('-'*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base model\n",
    "etr = ensemble.ExtraTreesRegressor(random_state=2, n_jobs=-1)\n",
    "base_results = model_selection.cross_validate(etr, df[x], df[y], cv=cv_split, n_jobs=-1)\n",
    "etr.fit(df[x], df[y])\n",
    "\n",
    "\n",
    "print('BEFORE Parameters: ', etr.get_params())\n",
    "print(\"BEFORE Training score mean: {:.2f}\". format(base_results['train_score'].mean()*100)) \n",
    "print(\"BEFORE Test score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\n",
    "print(\"BEFORE Test score 3*std: +/- {:.2f}\". format(base_results['test_score'].std()*100*3))\n",
    "print(\"BEFORE Test set score min: {:.2f}\". format(base_results['test_score'].min()*100))\n",
    "print('-'*10)\n",
    "\n",
    "\n",
    "#tune hyper-parameters: \n",
    "param_grid = {\n",
    "                'n_estimators': [1800],\n",
    "                'max_features': [.5],\n",
    "                'min_samples_split': [8],\n",
    "                'min_samples_leaf': [1],\n",
    "                'bootstrap': [True],\n",
    "                'random_state': [2]\n",
    "             }\n",
    "\n",
    "\n",
    "# choose best model with grid_search: \n",
    "tune_model = model_selection.GridSearchCV(etr, param_grid=param_grid, cv=cv_split, n_jobs=-1)\n",
    "tune_model.fit(df[x], df[y])\n",
    "\n",
    "print('AFTER Parameters: ', tune_model.best_params_)\n",
    "print(\"AFTER Training score mean: {:.2f}\". format(tune_model.cv_results_['mean_train_score'][tune_model.best_index_]*100)) \n",
    "print(\"AFTER Test score mean: {:.2f}\". format(tune_model.cv_results_['mean_test_score'][tune_model.best_index_]*100))\n",
    "print(\"AFTER Test score 3*std: +/- {:.2f}\". format(tune_model.cv_results_['std_test_score'][tune_model.best_index_]*100*3))\n",
    "print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature tuning\n",
    "\n",
    "#base model\n",
    "etr = ensemble.ExtraTreesRegressor(bootstrap=True, \n",
    "                                   max_features=0.5, \n",
    "                                   min_samples_split=8, \n",
    "                                   n_estimators=1800,\n",
    "                                   random_state=2, n_jobs=-1)\n",
    "base_results = model_selection.cross_validate(etr, df[x], df[y], cv=cv_split, n_jobs=-1)\n",
    "etr.fit(df[x], df[y])\n",
    "\n",
    "print('BEFORE RFE Training Shape Old: ', df[x].shape) \n",
    "print('BEFORE RFE Training Columns Old: ', df[x].columns.values)\n",
    "\n",
    "print(\"BEFORE RFE Training score mean: {:.2f}\". format(base_results['train_score'].mean()*100)) \n",
    "print(\"BEFORE RFE Test score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\n",
    "print(\"BEFORE RFE Test score 3*std: +/- {:.2f}\". format(base_results['test_score'].std()*100*3))\n",
    "print('-'*10)\n",
    "\n",
    "#feature selection\n",
    "etr_rfe = feature_selection.RFECV(etr, step = 1, cv = cv_split, n_jobs=-1)\n",
    "etr_rfe.fit(df[x], df[y])\n",
    "\n",
    "#transform x&y to reduced features and fit new model\n",
    "X_rfe = df[x].columns.values[etr_rfe.get_support()]\n",
    "rfe_results = model_selection.cross_validate(etr, df[X_rfe], df[y], cv  = cv_split, n_jobs=-1)\n",
    "\n",
    "print('AFTER RFE Training Shape New: ', df[X_rfe].shape) \n",
    "print('AFTER RFE Training Columns New: ', X_rfe)\n",
    "\n",
    "print(\"AFTER RFE Training score mean: {:.2f}\". format(rfe_results['train_score'].mean()*100)) \n",
    "print(\"AFTER RFE Test score mean: {:.2f}\". format(rfe_results['test_score'].mean()*100))\n",
    "print(\"AFTER RFE Test score 3*std: +/- {:.2f}\". format(rfe_results['test_score'].std()*100*3))\n",
    "print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etr_x = ['age_DRtg', 'age_ORtg', 'age_PER', 'age_PPR', 'age_STL%', 'age_BPM',\n",
    " 'age_MIN:GP', 'age_USGxTS', 'age_BLK%_ppctl', 'age_DRB%_ppctl',\n",
    " 'age_DRtg_ppctl', 'age_ORtg_ppctl', 'age_PPR_ppctl', 'age_PPS_ppctl',\n",
    " 'age_STL%_ppctl', 'age_Total S %_ppctl', 'age_eFG%_ppctl', 'age_BPM_ppctl',\n",
    " 'age_STLK%_ppctl', 'age_PF:STLK_ppctl', 'Height', 'Height_ppctl', 'Age',\n",
    " 'age_pDBPM_ppctl', 'age_PER_ppctl']\n",
    "\n",
    "#base model\n",
    "etr = ensemble.ExtraTreesRegressor(random_state=2, n_jobs=-1)\n",
    "base_results = model_selection.cross_validate(etr, df[etr_x], df[y], cv=cv_split, n_jobs=-1)\n",
    "etr.fit(df[etr_x], df[y])\n",
    "\n",
    "\n",
    "print('BEFORE Parameters: ', etr.get_params())\n",
    "print(\"BEFORE Training score mean: {:.2f}\". format(base_results['train_score'].mean()*100)) \n",
    "print(\"BEFORE Test score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\n",
    "print(\"BEFORE Test score 3*std: +/- {:.2f}\". format(base_results['test_score'].std()*100*3))\n",
    "print(\"BEFORE Test set score min: {:.2f}\". format(base_results['test_score'].min()*100))\n",
    "print('-'*10)\n",
    "\n",
    "\n",
    "#tune hyper-parameters: \n",
    "param_grid = {\n",
    "                'n_estimators': [1800],\n",
    "                'max_features': [.55, .6],\n",
    "                'min_samples_split': [7, 8],\n",
    "                'bootstrap': [True],\n",
    "                'random_state': [2]\n",
    "             }\n",
    "\n",
    "\n",
    "# choose best model with grid_search: \n",
    "tune_model = model_selection.GridSearchCV(etr, param_grid=param_grid, cv=cv_split, n_jobs=-1)\n",
    "tune_model.fit(df[etr_x], df[y])\n",
    "\n",
    "print('AFTER Parameters: ', tune_model.best_params_)\n",
    "print(\"AFTER Training score mean: {:.2f}\". format(tune_model.cv_results_['mean_train_score'][tune_model.best_index_]*100)) \n",
    "print(\"AFTER Test score mean: {:.2f}\". format(tune_model.cv_results_['mean_test_score'][tune_model.best_index_]*100))\n",
    "print(\"AFTER Test score 3*std: +/- {:.2f}\". format(tune_model.cv_results_['std_test_score'][tune_model.best_index_]*100*3))\n",
    "print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etr_x = ['age_DRtg', 'age_ORtg', 'age_PER', 'age_PPR', 'age_STL%', 'age_BPM',\n",
    " 'age_MIN:GP', 'age_USGxTS', 'age_BLK%_ppctl', 'age_DRB%_ppctl',\n",
    " 'age_DRtg_ppctl', 'age_ORtg_ppctl', 'age_PPR_ppctl', 'age_PPS_ppctl',\n",
    " 'age_STL%_ppctl', 'age_Total S %_ppctl', 'age_eFG%_ppctl', 'age_BPM_ppctl',\n",
    " 'age_STLK%_ppctl', 'age_PF:STLK_ppctl', 'Height', 'Height_ppctl', 'Age',\n",
    " 'age_pDBPM_ppctl', 'age_PER_ppctl']\n",
    "\n",
    "#final model\n",
    "etr = ensemble.ExtraTreesRegressor(bootstrap=True, \n",
    "                                   max_features=0.55, \n",
    "                                   min_samples_split=7, \n",
    "                                   n_estimators=1800,\n",
    "                                   random_state=2, n_jobs=-1)\n",
    "base_results = model_selection.cross_validate(etr, df[etr_x], df[y], cv=cv_split, n_jobs=-1)\n",
    "etr.fit(df[etr_x], df[y])\n",
    "\n",
    "\n",
    "print(\"Training score mean: {:.2f}\". format(base_results['train_score'].mean()*100)) \n",
    "print(\"Test score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\n",
    "print(\"Test score 3*std: +/- {:.2f}\". format(base_results['test_score'].std()*100*3))\n",
    "print('-'*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blend Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from mlxtend.regressor import StackingCVRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "random_state = 2\n",
    "np.random.seed(random_state)\n",
    "\n",
    "# models before feature tuning\n",
    "etr = ensemble.ExtraTreesRegressor(bootstrap=True, \n",
    "                                   max_features=0.5, \n",
    "                                   min_samples_split=8, \n",
    "                                   n_estimators=1800,\n",
    "                                   random_state=2, n_jobs=-1)\n",
    "\n",
    "rfr = ensemble.RandomForestRegressor(bootstrap=True, \n",
    "                                     max_features=0.3, \n",
    "                                     max_leaf_nodes=18,\n",
    "                                     min_samples_split=12, \n",
    "                                     n_estimators=800,\n",
    "                                     random_state=2, n_jobs=-1)\n",
    "\n",
    "ada = ensemble.AdaBoostRegressor(tree.DecisionTreeRegressor(max_depth=5, max_features=0.2), n_estimators=60)\n",
    "\n",
    "bag = ensemble.BaggingRegressor(ensemble.GradientBoostingRegressor(n_estimators=125, max_depth=3, min_samples_split=7),\n",
    "                                n_estimators=35,\n",
    "                                max_samples=.4,\n",
    "                                bootstrap_features=False)\n",
    "\n",
    "xgb = XGBRegressor(colsample_bylevel=0.25, colsample_bytree=0.25, max_depth=4, n_estimators=30)\n",
    "\n",
    "grb = ensemble.GradientBoostingRegressor(max_depth=3, min_samples_leaf=10, n_estimators=40, max_features=.1, subsample=.75)\n",
    "\n",
    "rdg = linear_model.RidgeCV(alphas=(175.0,))\n",
    "\n",
    "meta_regr = ensemble.RandomForestRegressor()\n",
    "\n",
    "models = [etr, rfr, ada, bag, xgb, grb, rdg]\n",
    "\n",
    "stack = StackingCVRegressor(regressors=models,\n",
    "                            meta_regressor=meta_regr)\n",
    "\n",
    "models_plus_stack = models + [stack]\n",
    "\n",
    "x = ['age_DRtg', 'age_ORtg', 'age_PER', 'age_PPR', 'age_STL%', 'age_BPM',\n",
    " 'age_MIN:GP', 'age_USGxTS', 'age_BLK%_ppctl', 'age_DRB%_ppctl',\n",
    " 'age_DRtg_ppctl', 'age_ORtg_ppctl', 'age_PPR_ppctl', 'age_PPS_ppctl',\n",
    " 'age_STL%_ppctl', 'age_Total S %_ppctl', 'age_eFG%_ppctl', 'age_BPM_ppctl',\n",
    " 'age_STLK%_ppctl', 'age_PF:STLK_ppctl', 'Height', 'Height_ppctl', 'Age',\n",
    " 'age_pDBPM_ppctl', 'age_PER_ppctl']\n",
    "X = np.array(df[x])\n",
    "y = np.array(df['ep_PIPM'].tolist())\n",
    "\n",
    "\n",
    "print('5-fold cross validation scores:\\n')\n",
    "\n",
    "for clf, label in zip(models_plus_stack, ['ExtraTrees', 'RandomForest', 'AdaBoost', 'Bagging', \n",
    "                                          'XGBoost', 'GradientBoost', 'RidgeRegr', 'StackingCVRegressor']):\n",
    "    scores = cross_val_score(clf, X, y, cv=5)\n",
    "    print(\"R^2 Score: %0.2f (+/- %0.2f) [%s]\" % (\n",
    "        scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
